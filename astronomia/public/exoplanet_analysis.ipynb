{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEKct0LW_bvh",
        "outputId": "f0374c35-3520-425f-cf20-9490571ec322"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lightkurve\n",
            "  Downloading lightkurve-2.5.1-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting astroquery\n",
            "  Downloading astroquery-0.4.11-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: astropy in /usr/local/lib/python3.12/dist-packages (7.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: beautifulsoup4>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (4.13.5)\n",
            "Requirement already satisfied: bokeh>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (3.7.3)\n",
            "Collecting fbpca>=1.0 (from lightkurve)\n",
            "  Downloading fbpca-1.0.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib>=3.1 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (3.10.0)\n",
            "Collecting memoization>=0.3.1 (from lightkurve)\n",
            "  Downloading memoization-0.4.0.tar.gz (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: patsy>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (1.0.1)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (2.32.4)\n",
            "Collecting s3fs>=2024.6.1 (from lightkurve)\n",
            "  Downloading s3fs-2025.9.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (1.16.2)\n",
            "Requirement already satisfied: tqdm>=4.25.0 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (4.67.1)\n",
            "Collecting uncertainties>=3.1.4 (from lightkurve)\n",
            "  Downloading uncertainties-3.2.3-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: urllib3>=1.23 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (2.5.0)\n",
            "Requirement already satisfied: html5lib>=0.999 in /usr/local/lib/python3.12/dist-packages (from astroquery) (1.1)\n",
            "Requirement already satisfied: keyring>=15.0 in /usr/local/lib/python3.12/dist-packages (from astroquery) (25.6.0)\n",
            "Collecting pyvo>=1.5 (from astroquery)\n",
            "  Downloading pyvo-1.7-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: pyerfa>=2.0.1.1 in /usr/local/lib/python3.12/dist-packages (from astropy) (2.0.1.5)\n",
            "Requirement already satisfied: astropy-iers-data>=0.2025.4.28.0.37.27 in /usr/local/lib/python3.12/dist-packages (from astropy) (0.2025.9.15.0.37.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from astropy) (6.0.2)\n",
            "Requirement already satisfied: packaging>=22.0.0 in /usr/local/lib/python3.12/dist-packages (from astropy) (25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.6.0->lightkurve) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.6.0->lightkurve) (4.15.0)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.12/dist-packages (from bokeh>=2.3.2->lightkurve) (3.1.6)\n",
            "Requirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.12/dist-packages (from bokeh>=2.3.2->lightkurve) (1.3.3)\n",
            "Requirement already satisfied: narwhals>=1.13 in /usr/local/lib/python3.12/dist-packages (from bokeh>=2.3.2->lightkurve) (2.5.0)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from bokeh>=2.3.2->lightkurve) (11.3.0)\n",
            "Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.12/dist-packages (from bokeh>=2.3.2->lightkurve) (6.4.2)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.12/dist-packages (from bokeh>=2.3.2->lightkurve) (2025.4.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.12/dist-packages (from html5lib>=0.999->astroquery) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from html5lib>=0.999->astroquery) (0.5.1)\n",
            "Requirement already satisfied: SecretStorage>=3.2 in /usr/local/lib/python3.12/dist-packages (from keyring>=15.0->astroquery) (3.4.0)\n",
            "Requirement already satisfied: jeepney>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from keyring>=15.0->astroquery) (0.9.0)\n",
            "Requirement already satisfied: jaraco.classes in /usr/local/lib/python3.12/dist-packages (from keyring>=15.0->astroquery) (3.4.0)\n",
            "Requirement already satisfied: jaraco.functools in /usr/local/lib/python3.12/dist-packages (from keyring>=15.0->astroquery) (4.3.0)\n",
            "Requirement already satisfied: jaraco.context in /usr/local/lib/python3.12/dist-packages (from keyring>=15.0->astroquery) (6.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1->lightkurve) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1->lightkurve) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1->lightkurve) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1->lightkurve) (3.2.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->lightkurve) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->lightkurve) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->lightkurve) (2025.8.3)\n",
            "Collecting aiobotocore<3.0.0,>=2.5.4 (from s3fs>=2024.6.1->lightkurve)\n",
            "  Downloading aiobotocore-2.24.2-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting fsspec==2025.9.0 (from s3fs>=2024.6.1->lightkurve)\n",
            "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from s3fs>=2024.6.1->lightkurve) (3.12.15)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.0->lightkurve) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.0->lightkurve) (3.6.0)\n",
            "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve)\n",
            "  Downloading aioitertools-0.12.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting botocore<1.40.19,>=1.40.15 (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve)\n",
            "  Downloading botocore-1.40.18-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: multidict<7.0.0,>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve) (6.6.4)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.12/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve) (1.17.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve) (1.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from Jinja2>=2.9->bokeh>=2.3.2->lightkurve) (3.0.2)\n",
            "Requirement already satisfied: cryptography>=2.0 in /usr/local/lib/python3.12/dist-packages (from SecretStorage>=3.2->keyring>=15.0->astroquery) (43.0.3)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from jaraco.classes->keyring>=15.0->astroquery) (10.8.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring>=15.0->astroquery) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring>=15.0->astroquery) (2.23)\n",
            "Downloading lightkurve-2.5.1-py3-none-any.whl (256 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.9/256.9 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astroquery-0.4.11-py3-none-any.whl (11.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyvo-1.7-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3fs-2025.9.0-py3-none-any.whl (30 kB)\n",
            "Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.3/199.3 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uncertainties-3.2.3-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiobotocore-2.24.2-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aioitertools-0.12.0-py3-none-any.whl (24 kB)\n",
            "Downloading botocore-1.40.18-py3-none-any.whl (14.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m134.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Building wheels for collected packages: fbpca, memoization\n",
            "  Building wheel for fbpca (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fbpca: filename=fbpca-1.0-py3-none-any.whl size=11373 sha256=b95f4ee2b1672661238353b62d48f04698e26b64c01f3573b8ff39212111c6aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/15/cd/2f622795b09e83471a3be5d2581cd9cf96a6ec7aa78e8deffe\n",
            "  Building wheel for memoization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for memoization: filename=memoization-0.4.0-py3-none-any.whl size=50452 sha256=44968deddc35a6dc6cf7fe17b9b793252fbdb6998eeb6e55d5aa8193e0ae443b\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/35/02/90618fc7cbf03a335f3cacd59d32b35930bf5a57f3c0d0814c\n",
            "Successfully built fbpca memoization\n",
            "Installing collected packages: fbpca, uncertainties, memoization, jmespath, fsspec, aioitertools, botocore, pyvo, aiobotocore, s3fs, astroquery, lightkurve\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.9.0 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiobotocore-2.24.2 aioitertools-0.12.0 astroquery-0.4.11 botocore-1.40.18 fbpca-1.0 fsspec-2025.9.0 jmespath-1.0.1 lightkurve-2.5.1 memoization-0.4.0 pyvo-1.7 s3fs-2025.9.0 uncertainties-3.2.3\n"
          ]
        }
      ],
      "source": [
        "!pip install lightkurve astroquery astropy numpy pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# exoplanet_classifier.py\n",
        "# Python 3.10+\n",
        "# Requirements:\n",
        "#   pip install astroquery astropy pandas scikit-learn numpy\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from astroquery.nasa_exoplanet_archive import NasaExoplanetArchive\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.metrics import f1_score, balanced_accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# 1) UTILITAIRES DOWNLOAD\n",
        "# --------------------------\n",
        "\n",
        "def df_from_table(table):\n",
        "    df = table.to_pandas()\n",
        "    df.columns = (\n",
        "        df.columns\n",
        "        .str.strip()\n",
        "        .str.replace(r\"\\s+\", \"_\", regex=True)\n",
        "        .str.replace(r\"[()]+\", \"\", regex=True)\n",
        "    )\n",
        "    return df\n",
        "\n",
        "\n",
        "def fetch_koi():\n",
        "    t = NasaExoplanetArchive.query_criteria(\n",
        "        table=\"koi\",\n",
        "        select=\"kepid, kepoi_name, koi_disposition, koi_period, koi_duration, koi_depth, koi_model_snr, \"\n",
        "               \"koi_steff, koi_slogg, koi_srad, koi_kepmag, koi_fpflag_nt, koi_fpflag_ss, koi_fpflag_co, koi_fpflag_ec\",\n",
        "        cache=True\n",
        "    )\n",
        "    df = df_from_table(t)\n",
        "    df[\"mission\"] = \"KEPLER\"\n",
        "    return df\n",
        "\n",
        "\n",
        "def fetch_k2():\n",
        "    \"\"\"\n",
        "    K2 Planets and Candidates (table = 'k2pandc').\n",
        "    Colonnes PS: pl_orbper (jours), pl_trandep (ppm), pl_trandur (jours), st_*, sy_*.\n",
        "    \"\"\"\n",
        "    from astroquery.nasa_exoplanet_archive import NasaExoplanetArchive\n",
        "\n",
        "    t = NasaExoplanetArchive.query_criteria(\n",
        "        table=\"k2pandc\",\n",
        "        select=(\n",
        "            \"epic_hostname, k2_name, hostname, disposition, \"\n",
        "            \"pl_orbper, pl_trandep, pl_trandur, \"\n",
        "            \"st_teff, st_logg, st_rad, \"\n",
        "            \"sy_kepmag, sy_tmag\"\n",
        "        ),\n",
        "        cache=True\n",
        "    )\n",
        "    df = t.to_pandas()\n",
        "    df.columns = (\n",
        "        df.columns\n",
        "        .str.strip()\n",
        "        .str.replace(r\"\\s+\", \"_\", regex=True)\n",
        "        .str.replace(r\"[()]+\", \"\", regex=True)\n",
        "    )\n",
        "\n",
        "    df.rename(columns={\n",
        "        \"epic_hostname\": \"epic_hostname\",\n",
        "        \"disposition\": \"Archive_Disposition\",\n",
        "        \"pl_orbper\": \"period\",\n",
        "        \"pl_trandur\": \"duration_d\",\n",
        "        \"pl_trandep\": \"depth\",\n",
        "        \"st_teff\": \"st_teff\",\n",
        "        \"st_logg\": \"st_logg\",\n",
        "        \"st_rad\": \"st_rad\",\n",
        "        \"sy_kepmag\": \"kepmag\",\n",
        "        \"sy_tmag\": \"tmag\",\n",
        "    }, inplace=True)\n",
        "\n",
        "    df[\"object_id\"] = (\n",
        "        df.get(\"epic_hostname\")\n",
        "          .fillna(df.get(\"k2_name\"))\n",
        "          .fillna(df.get(\"hostname\"))\n",
        "          .astype(str)\n",
        "    )\n",
        "\n",
        "    df[\"star_id\"] = df.get(\"epic_hostname\").fillna(df.get(\"hostname\")).astype(str)\n",
        "\n",
        "    # magnitude unique\n",
        "    df[\"mag\"] = df.get(\"kepmag\").fillna(df.get(\"tmag\"))\n",
        "\n",
        "    df[\"duration_h\"] = df[\"duration_d\"] * 24.0\n",
        "\n",
        "    df[\"mission\"] = \"K2\"\n",
        "    return df\n",
        "\n",
        "def fetch_toi():\n",
        "    \"\"\"\n",
        "    TESS Objects of Interest (table = 'toi').\n",
        "    Colonnes clefs (PS schema):\n",
        "      - tid (TIC ID), tfopwg_disp (disposition),\n",
        "      - pl_orbper [days], pl_trandurh [hours], pl_trandep [ppm],\n",
        "      - st_teff, st_logg, st_rad, st_tmag (TESS magnitude).\n",
        "    Doc colonnes: https://exoplanetarchive.ipac.caltech.edu/docs/API_TOI_columns.html\n",
        "    \"\"\"\n",
        "    from astroquery.nasa_exoplanet_archive import NasaExoplanetArchive\n",
        "\n",
        "    t = NasaExoplanetArchive.query_criteria(\n",
        "        table=\"toi\",\n",
        "        select=(\n",
        "            \"toi, tid, tfopwg_disp, \"\n",
        "            \"pl_orbper, pl_trandurh, pl_trandep, \"\n",
        "            \"st_teff, st_logg, st_rad, st_tmag\"\n",
        "        ),\n",
        "        cache=True\n",
        "    )\n",
        "    df = t.to_pandas()\n",
        "    df.columns = (\n",
        "        df.columns\n",
        "        .str.strip()\n",
        "        .str.replace(r\"\\s+\", \"_\", regex=True)\n",
        "        .str.replace(r\"[()]+\", \"\", regex=True)\n",
        "    )\n",
        "\n",
        "    # Renommage vers notre schéma commun\n",
        "    df.rename(columns={\n",
        "        \"tid\": \"TIC_ID\",\n",
        "        \"tfopwg_disp\": \"TFOPWG_Disposition\",\n",
        "        \"pl_orbper\": \"Period_days\",\n",
        "        \"pl_trandurh\": \"Duration_hours\",\n",
        "        \"pl_trandep\": \"Depth_ppm\",\n",
        "        \"st_tmag\": \"TESS_Mag\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    df[\"mission\"] = \"TESS\"\n",
        "    return df\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# 2) HARMONISATION\n",
        "# --------------------------\n",
        "\n",
        "def harmonize_koi(df):\n",
        "    return pd.DataFrame({\n",
        "        \"object_id\": df.get(\"kepoi_name\", df.index.astype(str)),\n",
        "        \"mission\": \"KEPLER\",\n",
        "        \"star_id\": df.get(\"kepid\"),\n",
        "        \"period\": df.get(\"koi_period\"),\n",
        "        \"duration\": df.get(\"koi_duration\"),\n",
        "        \"depth\": df.get(\"koi_depth\"),\n",
        "        \"snr\": df.get(\"koi_model_snr\"),\n",
        "        \"st_teff\": df.get(\"koi_steff\"),\n",
        "        \"st_logg\": df.get(\"koi_slogg\"),\n",
        "        \"st_rad\": df.get(\"koi_srad\"),\n",
        "        \"mag\": df.get(\"koi_kepmag\"),\n",
        "        \"fpflag_nt\": df.get(\"koi_fpflag_nt\"),\n",
        "        \"fpflag_ss\": df.get(\"koi_fpflag_ss\"),\n",
        "        \"fpflag_co\": df.get(\"koi_fpflag_co\"),\n",
        "        \"fpflag_ec\": df.get(\"koi_fpflag_ec\"),\n",
        "        \"label_raw\": df.get(\"koi_disposition\"),\n",
        "    })\n",
        "\n",
        "\n",
        "def harmonize_k2(df):\n",
        "    return pd.DataFrame({\n",
        "        \"object_id\": df.get(\"object_id\").astype(str),\n",
        "        \"mission\": \"K2\",\n",
        "        \"star_id\": df.get(\"star_id\").astype(str),\n",
        "        \"period\": df.get(\"period\"),\n",
        "        \"duration\": df.get(\"duration_h\"),\n",
        "        \"depth\": df.get(\"depth\"),\n",
        "        \"snr\": df.get(\"snr\"),\n",
        "        \"st_teff\": df.get(\"st_teff\"),\n",
        "        \"st_logg\": df.get(\"st_logg\"),\n",
        "        \"st_rad\": df.get(\"st_rad\"),\n",
        "        \"mag\": df.get(\"mag\"),\n",
        "        \"fpflag_nt\": df.get(\"fpflag_nt\"),\n",
        "        \"fpflag_ss\": df.get(\"fpflag_ss\"),\n",
        "        \"fpflag_co\": df.get(\"fpflag_co\"),\n",
        "        \"fpflag_ec\": df.get(\"fpflag_ec\"),\n",
        "        \"label_raw\": df.get(\"Archive_Disposition\"),\n",
        "    })\n",
        "\n",
        "\n",
        "\n",
        "def harmonize_toi(df):\n",
        "    def map_toi_label(x):\n",
        "        if pd.isna(x):\n",
        "            return None\n",
        "        x = str(x).upper()\n",
        "        if x in {\"KP\", \"CP\", \"CONFIRMED\"}:\n",
        "            return \"CONFIRMED\"\n",
        "        if x in {\"PC\", \"APC\", \"CANDIDATE\"}:\n",
        "            return \"CANDIDATE\"\n",
        "        if x in {\"FP\", \"FALSE POSITIVE\"}:\n",
        "            return \"FALSE POSITIVE\"\n",
        "        return None\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        \"object_id\": df.get(\"toi\").astype(str),\n",
        "        \"mission\": \"TESS\",\n",
        "        \"star_id\": df.get(\"TIC_ID\"),\n",
        "        \"period\": df.get(\"Period_days\"),\n",
        "        \"duration\": df.get(\"Duration_hours\"),\n",
        "        \"depth\": df.get(\"Depth_ppm\"),\n",
        "        \"snr\": df.get(\"SNR\"),\n",
        "        \"st_teff\": df.get(\"st_teff\"),\n",
        "        \"st_logg\": df.get(\"st_logg\"),\n",
        "        \"st_rad\": df.get(\"st_rad\"),\n",
        "        \"mag\": df.get(\"TESS_Mag\"),\n",
        "        \"fpflag_nt\": df.get(\"fpflag_nt\"),\n",
        "        \"fpflag_ss\": df.get(\"fpflag_ss\"),\n",
        "        \"fpflag_co\": df.get(\"fpflag_co\"),\n",
        "        \"fpflag_ec\": df.get(\"fpflag_ec\"),\n",
        "        \"label_raw\": df.get(\"TFOPWG_Disposition\").map(map_toi_label),\n",
        "    })\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# 3) PIPELINE ML\n",
        "# --------------------------\n",
        "\n",
        "def run_classifier(harm):\n",
        "    from sklearn.preprocessing import QuantileTransformer, OneHotEncoder\n",
        "    from sklearn.compose import ColumnTransformer\n",
        "    from sklearn.pipeline import Pipeline\n",
        "    from sklearn.impute import SimpleImputer\n",
        "    from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "    from sklearn.metrics import f1_score, balanced_accuracy_score\n",
        "    from sklearn.model_selection import StratifiedGroupKFold\n",
        "    from sklearn.utils.class_weight import compute_sample_weight\n",
        "\n",
        "    label_map = {\"CONFIRMED\": 2, \"CANDIDATE\": 1, \"FALSE POSITIVE\": 0}\n",
        "    harm = harm[harm[\"label_raw\"].isin(label_map.keys())].copy()\n",
        "    harm[\"label\"] = harm[\"label_raw\"].map(label_map).astype(int)\n",
        "\n",
        "    for c in [\"period\", \"duration\", \"depth\"]:\n",
        "        if c in harm.columns:\n",
        "            harm.loc[(~harm[c].isna()) & (harm[c].astype(float) <= 0), c] = np.nan\n",
        "\n",
        "    base_num_cols = [\n",
        "        \"period\", \"duration\", \"depth\", \"snr\",\n",
        "        \"st_teff\", \"st_logg\", \"st_rad\", \"mag\",\n",
        "        \"fpflag_nt\", \"fpflag_ss\", \"fpflag_co\", \"fpflag_ec\",\n",
        "    ]\n",
        "\n",
        "    eps = 1e-6\n",
        "    harm[\"log_period\"]          = np.log10(harm[\"period\"].astype(float) + eps)\n",
        "    harm[\"log_duration_h\"]      = np.log10(harm[\"duration\"].astype(float) + eps)\n",
        "    harm[\"log_depth_ppm\"]       = np.log10(harm[\"depth\"].astype(float) + eps)\n",
        "    harm[\"depth_over_duration\"] = harm[\"depth\"].astype(float) / (harm[\"duration\"].astype(float) + eps)\n",
        "\n",
        "    derived_cols = [\"log_period\", \"log_duration_h\", \"log_depth_ppm\", \"depth_over_duration\"]\n",
        "    base_num_cols = [c for c in base_num_cols if c in harm.columns]\n",
        "    all_num_cols = base_num_cols + derived_cols\n",
        "\n",
        "    cat_cols = [\"mission\"]\n",
        "\n",
        "    y = harm[\"label\"].values\n",
        "    groups_star = harm[\"star_id\"].astype(str).values\n",
        "    groups_mission = harm[\"mission\"].values\n",
        "\n",
        "    clf = HistGradientBoostingClassifier(\n",
        "        learning_rate=0.08,\n",
        "        max_iter=500,\n",
        "        max_leaf_nodes=31,\n",
        "        early_stopping=True,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    f1s, bals = [], []\n",
        "\n",
        "    for fold, (tr, te) in enumerate(cv.split(harm, y, groups_star), 1):\n",
        "        df_tr = harm.iloc[tr].copy()\n",
        "        df_te = harm.iloc[te].copy()\n",
        "\n",
        "        num_cols_fit = [c for c in all_num_cols if df_tr[c].notna().any()]\n",
        "        pre = ColumnTransformer([\n",
        "            (\"num\", Pipeline([\n",
        "                (\"imp\", SimpleImputer(strategy=\"median\")),\n",
        "                (\"qt\",  QuantileTransformer(output_distribution=\"normal\",\n",
        "                                            subsample=200_000, random_state=42)),\n",
        "            ]), num_cols_fit),\n",
        "            (\"cat\", Pipeline([\n",
        "                (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "                (\"oh\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "            ]), cat_cols),\n",
        "        ])\n",
        "\n",
        "        pipe = Pipeline([(\"pre\", pre), (\"clf\", clf)])\n",
        "        sw = compute_sample_weight(class_weight=\"balanced\", y=y[tr])\n",
        "        pipe.fit(df_tr[num_cols_fit + cat_cols], y[tr], clf__sample_weight=sw)\n",
        "        y_hat = pipe.predict(df_te[num_cols_fit + cat_cols])\n",
        "\n",
        "        f1s.append(f1_score(y[te], y_hat, average=\"macro\"))\n",
        "        bals.append(balanced_accuracy_score(y[te], y_hat))\n",
        "        print(f\"[Fold {fold}] F1-macro={f1s[-1]:.3f}, BalAcc={bals[-1]:.3f}\")\n",
        "\n",
        "    print(f\"\\nCV results: F1-macro={np.mean(f1s):.3f}±{np.std(f1s):.3f}, \"\n",
        "          f\"BalAcc={np.mean(bals):.3f}±{np.std(bals):.3f}\")\n",
        "\n",
        "    # ---------- Leave-One-Mission-Out () ----------\n",
        "    print(\"\\nLeave-One-Mission-Out:\")\n",
        "    for m in np.unique(groups_mission):\n",
        "        tr_idx = np.where(groups_mission != m)[0]\n",
        "        te_idx = np.where(groups_mission == m)[0]\n",
        "        if len(te_idx) < 50:\n",
        "            continue\n",
        "\n",
        "        df_tr = harm.iloc[tr_idx].copy()\n",
        "        df_te = harm.iloc[te_idx].copy()\n",
        "\n",
        "        num_cols_fit = [c for c in all_num_cols if df_tr[c].notna().any()]\n",
        "\n",
        "        pre = ColumnTransformer([\n",
        "            (\"num\", Pipeline([\n",
        "                (\"imp\", SimpleImputer(strategy=\"median\")),\n",
        "                (\"qt\",  QuantileTransformer(output_distribution=\"normal\",\n",
        "                                            subsample=200_000, random_state=42)),\n",
        "            ]), num_cols_fit),\n",
        "        ])\n",
        "\n",
        "        pipe = Pipeline([(\"pre\", pre), (\"clf\", clf)])\n",
        "        sw = compute_sample_weight(class_weight=\"balanced\", y=y[tr_idx])\n",
        "\n",
        "        pipe.fit(df_tr[num_cols_fit], y[tr_idx], clf__sample_weight=sw)\n",
        "        y_hat = pipe.predict(df_te[num_cols_fit])\n",
        "\n",
        "        f1 = f1_score(y[te_idx], y_hat, average=\"macro\")\n",
        "        bal = balanced_accuracy_score(y[te_idx], y_hat)\n",
        "        print(f\"Train≠{m} → Test={m}: F1-macro={f1:.3f}, BalAcc={bal:.3f}\")\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# 4) MAIN\n",
        "# --------------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "    print(\"Fetching NASA catalogs via astroquery...\")\n",
        "    koi_df = fetch_koi()\n",
        "    k2_df = fetch_k2()\n",
        "    toi_df = fetch_toi()\n",
        "\n",
        "    koi_df.to_csv(\"data/cumulative_koi.csv\", index=False)\n",
        "    k2_df.to_csv(\"data/k2planets.csv\", index=False)\n",
        "    toi_df.to_csv(\"data/toi.csv\", index=False)\n",
        "\n",
        "    print(\"Harmonizing...\")\n",
        "    harm = pd.concat([\n",
        "        harmonize_koi(koi_df),\n",
        "        harmonize_k2(k2_df),\n",
        "        harmonize_toi(toi_df)\n",
        "    ], ignore_index=True)\n",
        "\n",
        "    num_cols_all = harm.select_dtypes(include=[np.number]).columns\n",
        "    nan_ratio = harm[num_cols_all].isna().mean()\n",
        "    cols_to_drop = nan_ratio[nan_ratio > 0.95].index.tolist()\n",
        "    if cols_to_drop:\n",
        "        print(f\"[Info] Dropping quasi-empty numeric columns (>95% NaN): {cols_to_drop}\")\n",
        "        harm.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "    harm.to_csv(\"data/exoplanets_harmonized.csv\", index=False)\n",
        "\n",
        "    print(f\"Harmonized dataset shape: {harm.shape}\")\n",
        "    print(harm[\"label_raw\"].value_counts())\n",
        "\n",
        "    print(\"\\nRunning classifier...\")\n",
        "    run_classifier(harm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzmmtN3EIjnL",
        "outputId": "e3dfda70-f6eb-40f3-f227-3a5ffbf4c0b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching NASA catalogs via astroquery...\n",
            "Harmonizing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3920245596.py:373: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  harm = pd.concat([\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Harmonized dataset shape: (66358, 16)\n",
            "label_raw\n",
            "CONFIRMED            24203\n",
            "FALSE POSITIVE       22655\n",
            "CANDIDATE            15308\n",
            "NOT DISPOSITIONED     4072\n",
            "REFUTED                 22\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Running classifier...\n",
            "[Fold 1] F1-macro=0.830, BalAcc=0.832\n",
            "[Fold 2] F1-macro=0.829, BalAcc=0.831\n",
            "[Fold 3] F1-macro=0.835, BalAcc=0.840\n",
            "[Fold 4] F1-macro=0.831, BalAcc=0.834\n",
            "[Fold 5] F1-macro=0.830, BalAcc=0.832\n",
            "\n",
            "CV results: F1-macro=0.831±0.002, BalAcc=0.834±0.003\n",
            "\n",
            "Leave-One-Mission-Out:\n",
            "Train≠K2 → Test=K2: F1-macro=0.254, BalAcc=0.357\n",
            "Train≠KEPLER → Test=KEPLER: F1-macro=0.346, BalAcc=0.398\n",
            "Train≠TESS → Test=TESS: F1-macro=0.384, BalAcc=0.402\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intra-mission (CV par étoile) :\n",
        "Le modèle fonctionne très bien → F1-macro ≈ 0.83, Balanced Accuracy ≈ 0.83.\n",
        "👉 Interprétation : bonne capacité de généralisation à l’intérieur d’une même mission (Kepler, K2, TESS).\n",
        "\n",
        "Cross-mission (LOMO) :\n",
        "Quand on entraîne sur 2 missions et qu’on teste sur la 3ᵉ, les performances chutent fortement (F1-macro ~0.25–0.38).\n",
        "👉 Interprétation : le modèle apprend surtout des indices spécifiques aux missions (bruit instrumental, magnitudes, snr, critères de catalogage) → il ne généralise pas vers un nouveau survey.\n",
        "\n",
        "Probleme :\n",
        "\n",
        "Si l’objectif est de classifier dans une mission donnée → pas de problème, on peut ignorer le LOMO.\n",
        "\n",
        "Si l’objectif est de construire un modèle universel inter-mission → il faut travailler sur des features physiques invariantes (ratios, normalisations par étoile, paramètres orbitaux/stellaires dérivés) et/ou envisager des techniques d’adaptation de domaine."
      ],
      "metadata": {
        "id": "TDw9MYHTgAkb"
      }
    }
  ]
}